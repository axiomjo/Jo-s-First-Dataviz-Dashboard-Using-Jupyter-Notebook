{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWtxZMkhxlg2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9wADwK78DCz"
   },
   "source": [
    "# Proyek Analisis Data: **Air Quality Dataset**\n",
    "- **Nama:** Josephine Dermawan\n",
    "- **Email:** jojo.josephined@gmail.com\n",
    "- **ID Dicoding:** axiomjo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eE0raob58DC0"
   },
   "source": [
    "## Menentukan Pertanyaan Bisnis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmQeQ5YF8DC0"
   },
   "source": [
    "- Seperti apa central tendency komposisi-polutan-dara di masing-masing stasiun dari 2013-2017?\n",
    "- Kapan polutan melonjak naik?\n",
    "- Area mana yang sebaiknya diberi warning berdasarkan prediksi tren komposisi polutan udaranya?\n",
    "\n",
    "---\n",
    "\n",
    "| kriteria  | detail  |\n",
    "|-----------|-----------|\n",
    "| S     | ttg polusi udara (particulate matter, exhaust kendaraan SO2\tNO2\tCO\tO3\t)   |\n",
    "| M     | komposisinya yg diukur    |\n",
    "| A    | warn yg polusinya parah   |\n",
    "| R    | kita gmw metong krn polusi udara   |\n",
    "| T    | buat skrg   |\n",
    "\n",
    "pola, prediksi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-z4QGlO8DC1"
   },
   "source": [
    "## Import Semua Packages/Library yang Digunakan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Key information users need to know to achieve their goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FVYwaObI8DC1"
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "\\(* u*) this part just renames the library so they'll be eaasier to call later wkwkwkwk\n",
    "\n",
    "pandas = dataframe\n",
    "\n",
    "numpy = arrays, math operations\n",
    "scipy = moar advanced math ops\n",
    "\n",
    "matplotlib = dataviz\n",
    "seaborn = dataviz\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as mpl\n",
    "import seaborn as sb\n",
    "\n",
    "import sqlalchemy as sq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_Sh51Xy8DC1",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXU2GBYu8DC1"
   },
   "source": [
    "### Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿  \n",
    "ambil data semua stasiun dari **Air Quality Dataset**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zjCBk1BI8DC1"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m area2_Ch \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m20quality/PRSA_Data_Changping_20130301-20170228.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m area3_Di \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m20quality/PRSA_Data_Dingling_20130301-20170228.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m area4_Do \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m20quality/PRSA_Data_Dongsi_20130301-20170228.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m area5_Ga \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m20quality/PRSA_Data_Guanyuan_20130301-20170228.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m area6_Gc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m20quality/PRSA_Data_Gucheng_20130301-20170228.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dicoding_analisis_data_airquality_jojo-JBwPaeKG/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dicoding_analisis_data_airquality_jojo-JBwPaeKG/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dicoding_analisis_data_airquality_jojo-JBwPaeKG/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dicoding_analisis_data_airquality_jojo-JBwPaeKG/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dicoding_analisis_data_airquality_jojo-JBwPaeKG/lib/python3.8/site-packages/pandas/io/common.py:716\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    713\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    725\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dicoding_analisis_data_airquality_jojo-JBwPaeKG/lib/python3.8/site-packages/pandas/io/common.py:373\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    371\u001b[0m             \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n\u001b[1;32m    372\u001b[0m             compression \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m--> 373\u001b[0m         reader \u001b[38;5;241m=\u001b[39m BytesIO(\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IOArgs(\n\u001b[1;32m    375\u001b[0m         filepath_or_buffer\u001b[38;5;241m=\u001b[39mreader,\n\u001b[1;32m    376\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m         mode\u001b[38;5;241m=\u001b[39mfsspec_mode,\n\u001b[1;32m    380\u001b[0m     )\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_fsspec_url(filepath_or_buffer):\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:472\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 472\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead:\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:613\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[1;32m    607\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 613\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[1;32m    615\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1270\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1267\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1268\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1269\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1128\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "tabelnya sama semua, cuma beda kota. :o\n",
    "\n",
    "for now manggil file csv yg udh ku-reupload ke githubku aja.\n",
    "ambil link dari tombol \"Raw\"\n",
    "'''\n",
    "\n",
    "area1_Ao = pd.read_csv(\"https://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air%20quality/PRSA_Data_Aotizhongxin_20130301-20170228.csv\")\n",
    "area2_Ch = pd.read_csv(\"https://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air%20quality/PRSA_Data_Changping_20130301-20170228.csv\")\n",
    "area3_Di = pd.read_csv(\"https://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air%20quality/PRSA_Data_Dingling_20130301-20170228.csv\")\n",
    "area4_Do = pd.read_csv(\"https://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air%20quality/PRSA_Data_Dongsi_20130301-20170228.csv\")\n",
    "area5_Ga = pd.read_csv(\"https://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air%20quality/PRSA_Data_Guanyuan_20130301-20170228.csv\")\n",
    "area6_Gc = pd.read_csv(\"https://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air%20quality/PRSA_Data_Gucheng_20130301-20170228.csv\")\n",
    "area7_Hu = pd.read_csv(\"https://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air%20quality/PRSA_Data_Huairou_20130301-20170228.csv\")\n",
    "area8_No = pd.read_csv(\"https://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air%20quality/PRSA_Data_Nongzhanguan_20130301-20170228.csv\")\n",
    "area9_Sh = pd.read_csv(\"https://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air%20quality/PRSA_Data_Shunyi_20130301-20170228.csv\")\n",
    "area10_Ti = pd.read_csv(\"https://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air%20quality/PRSA_Data_Tiantan_20130301-20170228.csv\")\n",
    "area11_Wl = pd.read_csv(\"https://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air%20quality/PRSA_Data_Wanliu_20130301-20170228.csv\")\n",
    "area12_Ws = pd.read_csv(\"https://raw.githubusercontent.com/axiomjo/Jo-s-First-Dataviz-Dashboard-Using-Jupyter-Notebook/refs/heads/main/submission-dicoding-analisis-data/data/air%20quality/PRSA_Data_Wanshouxigong_20130301-20170228.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "supaya gampang, jadiin satu n tampilin\n",
    "\"\"\"\n",
    "data_semua_area = pd.concat([area1_Ao,area2_Ch,area3_Di,area4_Do,area5_Ga,area6_Gc,area7_Hu,area8_No,area9_Sh,area10_Ti,area11_Wl,area12_Ws]) \n",
    "data_semua_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_semua_area.station.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMi6xGaDkbCi"
   },
   "source": [
    "ðŸŒ± ðŸŒ¿ **Insight:**  ðŸŒ± ðŸŒ¿\n",
    "1. OH! ada **12 stasiun**  \n",
    "2. TRUS, periode waktunya **PLEK SAMA SEMUA** dari 2013-2017  \n",
    "3. udh digabung di dataframe <code> data_semua_area </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHSiqaZp8DC1",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿  \n",
    "1. liat isi dataframenya kayak apa \n",
    "2. cek masalah tipikal di data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”ðŸ”ðŸ” 1. liat isi dataframenya kayak apa ðŸ”ðŸ”ðŸ” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minta ingfo ttg jumlah per kolom masse (*-*)/\n",
    "data_semua_area.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_semua_area.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "ðŸ”ðŸ”ðŸ” 2. cek masalah tipikal di data ðŸ”ðŸ”ðŸ”  \n",
    "- **missing value**\n",
    "- invalid (... ini engga coz i dunno regex yet)\n",
    "- **duplicate**\n",
    "- inaccurate (...eh.. ini engga, aku pasrah sm data dari sensor)\n",
    "- inconsistent (...ini engga juga, dunno how to find out yet)\n",
    "- **outlier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ax-3tEjc9Cj1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(;0;) cek yg bermasalah:\n",
    "- missing value\n",
    "\"\"\"\n",
    "data_semua_area.isnull().sum() \n",
    "# bisa pake data_semua_area.isna().sum() juga.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒ± karena aku mblenek liat angka, sekalian di-dataviz-in aja ya walopun blom EDA ðŸŒ± "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# rumus matem nyari persentase\n",
    "data_yg_ilang = data_semua_area.isnull().sum()\n",
    "persentase_data_yg_ilang = (data_yg_ilang / len(data_semua_area)) * 100\n",
    "\n",
    "# bikin dataframe\n",
    "temp_dataframe = pd.DataFrame({\n",
    "    \"Missing Count\": data_yg_ilang,\n",
    "    \"Missing Percentage\": persentase_data_yg_ilang\n",
    "}).reset_index()\n",
    "\n",
    "# namain ulang kolomnya\n",
    "temp_dataframe.columns = [\"Nama Kolom\", \"Data Hilang\", \"Persentase Hilang\"]\n",
    "\n",
    "# display dataframe\n",
    "print(temp_dataframe)\n",
    "\n",
    "# dataviz diagram batang\n",
    "mpl.figure(figsize=(10, 5))\n",
    "mpl.barh(temp_dataframe[\"Nama Kolom\"], temp_dataframe[\"Persentase Hilang\"], color=\"salmon\")\n",
    "mpl.xlabel(\"Data yang hilang (%)\")\n",
    "mpl.ylabel(\"Nama Kolom\")\n",
    "mpl.title(\"(T-T) Persentase data yang hilang per kolom \")\n",
    "mpl.gca().invert_yaxis()  # Invert to match DataFrame order\n",
    "mpl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(;0;) cek yg bermasalah:\n",
    "- duplicate\n",
    "\"\"\"\n",
    "data_semua_area.duplicated().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒ± karena outlier keliatan pake Inter Quartile Range... sekalian dataviz aja supaya enak ðŸŒ± "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(;0;) cek yg bermasalah:\n",
    "- outlier\n",
    "\"\"\"\n",
    "#pake looping,\n",
    "#dataframe pandasnya convert ke numpy dulu (tapi sekalian di-drop yg null supaya g error)\n",
    "#trus simpen data per stasiun di dictionary krn indeksnya string\n",
    "header_tabel_data = [\"PM2.5\",\"PM10\",\"SO2\",\"NO2\",\"CO\",\"O3\"]\n",
    "masing_masing_data_per_header = {}\n",
    "for tiap_header_data in header_tabel_data:\n",
    "    masing_masing_data_per_header[tiap_header_data] = pd.to_numeric(data_semua_area[tiap_header_data], errors='coerce'  ).dropna().to_numpy()\n",
    "\n",
    "#subplot yg dibutuhin matplotlib\n",
    "banyak_header = len(masing_masing_data_per_header)  \n",
    "fig, axes = mpl.subplots(banyak_header, 1, figsize=(8, banyak_header * 3))\n",
    "\n",
    "#boxplot\n",
    "for idx, (header, data) in enumerate(masing_masing_data_per_header.items()):\n",
    "    axes[idx].boxplot(data, vert=False)  \n",
    "    axes[idx].set_title(header)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dtxhAPrkhPL"
   },
   "source": [
    "ðŸŒ± ðŸŒ¿ **Insight:** ðŸŒ± ðŸŒ¿\n",
    "\n",
    "\n",
    "(;0;) hasil cek yg bermasalah:\n",
    "- **missing value**  \n",
    "   buanyak. tiap polutan ada huhuhuhuhu\n",
    "\n",
    "| polutan  | banyak data  |\n",
    "|----------|--------------|\n",
    "| PM2.5  | 8739   |\n",
    "| PM10   | 6449    |\n",
    "| SO2    | 9021   |\n",
    "| NO2    | 12116   |\n",
    "| CO    | 20701   |\n",
    "| O3    | 13277   |\n",
    "      \n",
    "- **duplicate**  \n",
    "    gaada yey :>\n",
    "\n",
    "- **outlier**  \n",
    "    somehow banyak bgt outliernya ??? kyk bukan outlier yg layak dibuang. need further exploration.\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhN5R4hr8DC1"
   },
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿ðŸŒ± ðŸŒ¿  \n",
    "1. benerin missing value\n",
    "2. drop kolom yg gakepake\n",
    "3. benerin outlier\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒ± ðŸŒ¿ðŸŒ± 1. benerin missing value ðŸŒ± ðŸŒ¿ðŸŒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVnYpprE9Evz"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "problem #1 MISSING VALUE (T-T)\n",
    "karena dataku time series... keknya jgn di-drop, \n",
    "karena dataku ga kategoris... keknya jgn di-imputate,\n",
    "so.. interpolation it is.\n",
    "\n",
    "oh, trus kupindah ke dataframe baru.\n",
    "\"\"\"\n",
    "\n",
    "cleaned_data_semua_area = data_semua_area.interpolate(method='linear')\n",
    "cleaned_data_semua_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒ±cek beneran keisi ga wkwkwk, saia susðŸŒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_semua_area.isna().sum() #missing value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒ± ðŸŒ¿ðŸŒ± 2. drop kolom yg gakepake ðŸŒ± ðŸŒ¿ðŸŒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_semua_area.drop(['TEMP',\t'PRES',\t'DEWP',\t'RAIN',\t'wd',\t'WSPM'], axis=1, inplace=True)\n",
    "cleaned_data_semua_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒ± ðŸŒ¿ðŸŒ± 3. benerin outlier ðŸŒ± ðŸŒ¿ðŸŒ±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒ±eh... tbh nyariin outlier per stasiiun kyk tahap assessing data lagi? tapi krn sekalian dibersihin, jadi kumasukin CLEANING DATAðŸŒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "problem #2 OUTLIER (T-T)\n",
    "karena outlier lebih kontekstual klo per kota...\n",
    "i need to know per kota... \n",
    "\"\"\"\n",
    "\n",
    "#dapetin semua kategori\n",
    "key_stasiun_data = cleaned_data_semua_area[\"station\"].unique()\n",
    "header_tabel_data = [\"PM2.5\", \"PM10\", \"SO2\", \"NO2\", \"CO\", \"O3\"]\n",
    "\n",
    "#bikin subplot, jenis stasiun ke samping, jenis polutan ke bawah.\n",
    "fig, axes = mpl.subplots(len(key_stasiun_data), len(header_tabel_data),  figsize=(len(key_stasiun_data) * 5, len(header_tabel_data) * 3))\n",
    "\n",
    "outlier_data_semua_area = []\n",
    "\n",
    "#looping per stasiun, per polutan\n",
    "for i, tiap_key in enumerate(key_stasiun_data):\n",
    "    stasiun_data = cleaned_data_semua_area[cleaned_data_semua_area[\"station\"] == tiap_key]\n",
    "\n",
    "    for j, tiap_header_data in enumerate(header_tabel_data):\n",
    "        # Convert to numeric and remove NaN\n",
    "        data_per_header = pd.to_numeric(stasiun_data[tiap_header_data], errors='coerce').dropna().to_numpy()\n",
    "\n",
    "        Q1 = np.percentile(data_per_header, 25)\n",
    "        Q3 = np.percentile(data_per_header, 75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outlier_temp = data_per_header[(data_per_header < lower_bound) | (data_per_header > upper_bound)]\n",
    "\n",
    "        for tiap_data in outlier_temp:\n",
    "            outlier_data_semua_area.append({\"station\": tiap_key, \"pollutant\": tiap_header_data, \"outlier_value\": tiap_data})\n",
    "\n",
    "\n",
    "        #dataviz boxplot\n",
    "        axes[i, j].boxplot(data_per_header, vert=False)\n",
    "        axes[i,j].set_title(f\"{tiap_key} - {tiap_header_data}\")\n",
    "\n",
    "mpl.tight_layout()\n",
    "mpl.show()\n",
    "\n",
    "outlier_data_semua_area = pd.DataFrame(outlier_data_semua_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================================================  \n",
    "BARU SAMPE SINIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n",
    "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒ±daripada ngilangin data, kutandain aja di kolom barunya <code> cleaned_data_semua_area </code> ðŸŒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the \"outlier_kah\" column with 0\n",
    "cleaned_data_semua_area[\"outlier_kah\"] = 0\n",
    "\n",
    "# Step 2: Loop through each station and pollutant to mark outliers\n",
    "for i, row in outlier_data_semua_area.iterrows():\n",
    "    station = row[\"station\"]\n",
    "    pollutant = row[\"pollutant\"]\n",
    "    outlier_value = row[\"outlier_value\"]    # Find rows where the station and pollutant match and are in the outliers list\n",
    "    \n",
    "    mask = (cleaned_data_semua_area[\"station\"] == station) & (cleaned_data_semua_area[pollutant].isin(outlier_data_semua_area))\n",
    "    \n",
    "    # Set \"outlier_kah\" to 1 for those rows\n",
    "    cleaned_data_semua_area.loc[mask, \"outlier_kah\"] = 1\n",
    "\n",
    "cleaned_data_semua_area.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ðŸŒ±krn penasaran persentase outlier, sekalian datavizðŸŒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_outlier = outlier_data_semua_area.groupby([\"station\", \"pollutant\"]).size().reset_index(name=\"outlier_count\")\n",
    "\n",
    "### Step 4: Bar Plot\n",
    "mpl.figure(figsize=(12,6))\n",
    "sb.barplot(data=temp_outlier, x=\"station\", y=\"outlier_count\", hue=\"pollutant\")\n",
    "mpl.title(\"Outlier Count per Station and Pollutant\")\n",
    "mpl.xticks(rotation=45)\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "bersihin tambahan: convert kolom waktu jadi dataframe sekaligus index supaya pas bikin dataviz enak\n",
    "\"\"\"\n",
    "\n",
    "cleaned_data_semua_area['timestamp'] = pd.to_datetime(cleaned_data_semua_area[['year', 'month', 'day','hour']])\n",
    "cleaned_data_semua_area.set_index('timestamp', inplace=True)\n",
    "\n",
    "cleaned_data_semua_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_5ejIqckiSP"
   },
   "source": [
    "**Insight:**\n",
    "- OUTLIERNYA GAK KAYAK OUTLIER GASIII? terlalu banyak klo diilangin.\n",
    "- udah diambil yg perlu aja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp-Y6wU38DC1"
   },
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MW7WF2kr8DC1"
   },
   "source": [
    "### Explore \n",
    "- central tendency tiap polutan per kota \n",
    "- bentuk distribusi data per kota\n",
    "- pemetaan berdasarkan time axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9CQCZjk8DC2"
   },
   "outputs": [],
   "source": [
    "central_tendency_stasiun_polutan = cleaned_data_semua_area.groupby(by='station').agg({\n",
    "    'PM2.5': ['mean','max', 'min'],\n",
    "    'PM10': ['mean','max', 'min'],\n",
    "    'SO2': ['mean','max', 'min'],\n",
    "    'NO2': ['mean','max', 'min'],\n",
    "    'CO': ['mean','max', 'min'],\n",
    "    'O3': ['mean','max', 'min'],\n",
    "              \n",
    "}).reset_index()\n",
    "\n",
    "central_tendency_stasiun_polutan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(T-T) gakuat liat angkaaaaa, butuh datavizzzzzz\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, len(header_tabel_data), figsize=(20, n_rows * 4))\n",
    " \n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "for i, tiap_key in enumerate(key_stasiun_data):\n",
    "    stasiun_data = cleaned_data_semua_area[cleaned_data_semua_area[\"station\"] == tiap_key]\n",
    "\n",
    "    for j, tiap_header_data in enumerate(header_tabel_data):\n",
    "        \n",
    "\n",
    "# Menyesuaikan layout agar lebih rapi\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot data for heatmap\n",
    "heatmap_data = outlier_data_semua_area.pivot(index=\"Station\", columns=\"Pollutant\", values=\"Outlier Count\").fillna(0)\n",
    "\n",
    "# Plot heatmap\n",
    "mpl.figure(figsize=(10, 6))\n",
    "sb.heatmap(heatmap_data, cmap=\"Reds\", annot=True, fmt=\".0f\")\n",
    "mpl.title(\"Outlier Count Heatmap (Stations vs Pollutants)\")\n",
    "mpl.xlabel(\"Pollutant\")\n",
    "mpl.ylabel(\"Station\")\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.figure(figsize=(10, 6))\n",
    "sb.heatmap(central_tendency_stasiun_polutan.xs('mean', axis=1, level=1), annot=True, cmap=\"coolwarm\", fmt=\".1f\")\n",
    "mpl.title(\"Mean Pollutant Levels per Station\")\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "th_Lzl2Fkj9O"
   },
   "source": [
    "**Insight:**\n",
    "- xxx\n",
    "- xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsyZjqak8DC2"
   },
   "source": [
    "## Visualization & Explanatory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZxOiQ6n8DC2"
   },
   "source": [
    "### Pertanyaan 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1swJUdAD8DC2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgHI7CiU8DC2"
   },
   "source": [
    "### Pertanyaan 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Go0lCsvO8DC2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0-36BDLklRg"
   },
   "source": [
    "**Insight:**\n",
    "- xxx\n",
    "- xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9y4VUsmcYNZ5"
   },
   "source": [
    "## Analisis Lanjutan (Opsional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWhnzsJGYUCO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WeHlCeX8DC2"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTcyR48Y8DC2"
   },
   "source": [
    "- Conclution pertanyaan 1\n",
    "- Conclution pertanyaan 2"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "972b3bf27e332e87b5379f2791f6ef9dfc79c71018c370b0d7423235e20fe4d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
